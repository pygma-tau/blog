<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Diffusion Roundup</title>
    <link rel="stylesheet" href="https://yudhister.me/style.css">
    <!-- TikZJax for TikZ diagrams -->
    <link rel="stylesheet" href="https://tikzjax.com/v1/fonts.css">
    <script src="https://tikzjax.com/v1/tikzjax.js"></script>
    <!-- MathJax for LaTeX rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://yudhister.me/js/footnotes.js"></script>
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-TKY9S092B5"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-TKY9S092B5');
	</script>
    <script>
		window.MathJax = {
		  tex: {
			inlineMath: [['$','$'], ['\\(','\\)']],
			displayMath: [ ['$$','$$'], ["\\[","\\]"], ]
		  },
		  startup: {
			ready: () => {
					// Function to iterate over all pre and code elements
					// if they contain TeX/LaTeX code for maths as defined
					// by the markers in tex settings above then copy their
					// textContent before them and remove the element from
					// the DOM.

					// get pre and code elements
					var prelist = document.getElementsByTagName("pre");
					var codelist = document.getElementsByTagName("code");
					// get delimiters for inline and display math
					var inline = MathJax.config.tex.inlineMath;
					var display = MathJax.config.tex.displayMath;
					// start building  a RegExp for each of these math types
					var inlineRegexList = [];
					var displayRegexList =[];
					for(i=0;i<inline.length;i++) {
						// https://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
						delimLEsc = inline[i][0].replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
						//alert(delimLEsc);
						delimREsc = inline[i][1].replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
						inlineRegexList.push("("+delimLEsc+")((.|[\\r\\n\\t])*?)("+delimREsc+")");
					};
					for(i=0;i<display.length;i++) {
						// https://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
						delimLEsc = display[i][0].replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
						//alert(delimLEsc);
						delimREsc = display[i][1].replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
						displayRegexList.push("("+delimLEsc+")((.|[\\r\\n\\t])*?)("+delimREsc+")");
					};
					inlineRegExp = new RegExp(inlineRegexList.join("|"));
					displayRegExp = new RegExp(displayRegexList.join("|"));

					// iterate over pre elements applying RegExp
					// iterate "backwards" as we are removing elements!
					for (i=prelist.length; i>0; i--) {
						if(displayRegExp.test(prelist[i-1].textContent)) {
							var t = document.createTextNode(prelist[i-1].textContent);
							prelist[i-1].parentNode.insertBefore(t,prelist[i-1]);
							prelist[i-1].parentNode.removeChild(prelist[i-1]);
						}
					}
					// iterate over code elements applying RegExp
					// iterate "backwards" as we are removing elements!
					for (i=codelist.length; i>0; i--) {
						if(inlineRegExp.test(codelist[i-1].textContent)) {
							var t = document.createTextNode(codelist[i-1].textContent);
							codelist[i-1].parentNode.insertBefore(t,codelist[i-1]);
							codelist[i-1].parentNode.removeChild(codelist[i-1]);
						}
					}
			  // Now process the page in MathJax
			  MathJax.startup.defaultReady();
			}
		  }
		};
		</script>
		<script type="text/javascript" id="MathJax-script" async
		  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
		</script>
</head>
<body>
	<nav>
        <ul>
            <li>[<a href="/">Home</a>]</li>
            <li>[<a href="/about">About</a>]</li>
            <li>[<a href="/notes">Notes</a>]</li>
			<li>[<a href="/shelf">Shelf</a>]</li>
			<!-- <li>[<a href="/cities">Cities</a>]</li> -->
			<!-- <li>[<a href="/heroes">Heroes</a>]</li> -->
			<li>[<a href="mailto:yudhister.j.kumar@gmail.com">Email</a>]</li>
			<li>[<a href="/atom.xml">RSS</a>]</li>
    </nav>
    <main>
        <h1>Diffusion Roundup</h1>
        <p><i>September 24, 2025</i></p>
        <p>[1] <strong>Diffusion models seem to outperform traditional autoregressive models in the large data limit on token-prediction tasks.</strong> <sup class="footnote-reference"><a href="#1">1</a></sup> Autoregressive models are still superior in the low-data/compute-limited regime, and the threshold at which diffusion models become optimal follows a power-law in the dataset size (typically exceeding the Chinchilla threshold by a large margin).<sup class="footnote-reference"><a href="#2">2</a></sup> Diffusion models also see performance gains under “trivial” data augmentation methods for far longer than autoregressive models (e.g. reordering tokens), and this is plausibly because the generation method is fundamentally non-causal? (Much of the performance gap can be recovered by implementing similar data augmentation methods in the AR case, but it’s unclear if this scales to tasks that require “cognition” in the human sense of the word). Not entirely clear how this translates to better performance on real-world tasks in the data-limited regime; it could be that the compute scaling necessary is simply prohibitive, and it could also be that the implicit curriculum afforded by the de-noising process is simply insufficient at providing reasonable enough signal on difficult tasks.</p>
<p>[2] <strong>Diffusion in-practice probably has the circuit complexity depth constraints of an attention-based transformer.</strong> In the last few years, we’ve seen literature essentially claiming that attention in-practice is limited to modeling circuits in the class TC0 (polynomial width, constant depth Boolean circuit family)<sup class="footnote-reference"><a href="#3">3</a></sup> Adding chain-of-thought ~roughly increases this to NC1 (although there are some subtleties involving the lack of robustness to input-ordering).<sup class="footnote-reference"><a href="#4">4</a></sup> There are reasons to expect difficult problems, especially the sorts encountered in long-horizon RL, to require architectures that can internally simulate deep computation. These architectures have been recurrent thus far. However, recurrent architectures fail to adequately leverage the compute parallelism offered by GPUs and have many, many issues with unstable training dynamics, so scaling transformers is a better option. It’s probably not the case that diffusion models can prove an adequate replacement here, but it’s interesting that <a href="https://arxiv.org/abs/2507.12469">a diffusion process with no constraints imposed by a score function can theoretically simulate any Turing-complete process, but when perfectly matching a score function still has the limitations of a TC0 representation</a>. Results in the approximate regime pending.<sup class="footnote-reference"><a href="#5">5</a></sup></p>
<p>[3] <strong>Diffusion is (kind of) spectral autoregression.</strong><sup class="footnote-reference"><a href="#6">6</a></sup> There are <a href="https://sander.ai/2024/09/02/spectral-autoregression.html">two</a> <a href="https://www.fabianfalck.com/posts/spectralauto/">brilliant</a> blog-posts on the subject, cumulatively arguing that DDPM has an inductive bias to generating low-frequency features before high-frequency features (in Fourier space; hence the name) but this is not necessarily true of all possible diffusion models (changing the model’s noising schedule to be frequency-agnostic doesn’t degrade performance on CIFAR10 and similar datasets, but not all noising schedules achieve the same performance!). How much does this matter for text-data domains? Audio? Video? Are there correspondences we can make between distributional structure and optimal noising schedules? In algorithmic cases, what does this mean?</p>
<p>I primarily find diffusion models interesting from a theoretical perspective, given that the corresponding SDE literature is rich and there are (potentially) deep connections to be made to modern ML. In particular, I expect we can better understand what feature orderings are optimal, which properties of distributions make them learnable, how much an inductive bias is a property of the model architecture vs. optimization algorithm or other factors, and to what extent recurrence can be represented with parallel architectures. This post should not be taken as definitive; it has not been edited and I welcome feedback.</p>
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>This section is summarizing the paper <a href="https://arxiv.org/abs/2507.15857">Diffusion Beats Autoregression in Data-Constrained Settings</a>.</p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>The metric the authors use is “number of unique tokens”—which is quite strange, given that the vocab size of a model is typically quite limited, and they mention training a 2.3B parameter diffusion model on a 500M unique token dataset. Perhaps they mean just the token size of a dataset with no repeated entries?</p>
</div>
<div class="footnote-definition" id="3"><sup class="footnote-definition-label">3</sup>
<p>See <a href="https://arxiv.org/abs/2207.00729">The Parallelism Tradeoff: Limitations of Log-Precision Transformers</a>, <a href="https://arxiv.org/abs/2402.09268">Transformers, parallel computation, and logarithmic depth</a>, and <a href="https://arxiv.org/abs/2412.02975">Theoretical limitations of multi-layer Transformer</a>.</p>
</div>
<div class="footnote-definition" id="4"><sup class="footnote-definition-label">4</sup>
<p>See <a href="https://arxiv.org/abs/2402.12875">Chain of Thought Empowers Transformers to Solve Inherently Serial Problems</a>. CoT/neuralese introducing “effective recurrence” into modern models seems to be important for timeline modeling.</p>
</div>
<div class="footnote-definition" id="5"><sup class="footnote-definition-label">5</sup>
<p>Reach out if you have thoughts!</p>
</div>
<div class="footnote-definition" id="6"><sup class="footnote-definition-label">6</sup>
<p>See A Fourier Space Perspective on Diffusion Models.</p>
</div>

    </main>
</body>
</html>
