<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prediction is not Generation</title>
    <link rel="stylesheet" href="https://yudhister.me/style.css">
    <!-- TikZJax for TikZ diagrams -->
    <link rel="stylesheet" href="https://tikzjax.com/v1/fonts.css">
    <script src="https://tikzjax.com/v1/tikzjax.js"></script>
    <!-- MathJax for LaTeX rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://yudhister.me/js/footnotes.js"></script>
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-TKY9S092B5"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'G-TKY9S092B5');
	</script>
    <script>
		window.MathJax = {
		  tex: {
			inlineMath: [['$','$'], ['\\(','\\)']],
			displayMath: [ ['$$','$$'], ["\\[","\\]"], ]
		  },
		  startup: {
			ready: () => {
					// Function to iterate over all pre and code elements
					// if they contain TeX/LaTeX code for maths as defined
					// by the markers in tex settings above then copy their
					// textContent before them and remove the element from
					// the DOM.

					// get pre and code elements
					var prelist = document.getElementsByTagName("pre");
					var codelist = document.getElementsByTagName("code");
					// get delimiters for inline and display math
					var inline = MathJax.config.tex.inlineMath;
					var display = MathJax.config.tex.displayMath;
					// start building  a RegExp for each of these math types
					var inlineRegexList = [];
					var displayRegexList =[];
					for(i=0;i<inline.length;i++) {
						// https://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
						delimLEsc = inline[i][0].replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
						//alert(delimLEsc);
						delimREsc = inline[i][1].replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
						inlineRegexList.push("("+delimLEsc+")((.|[\\r\\n\\t])*?)("+delimREsc+")");
					};
					for(i=0;i<display.length;i++) {
						// https://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
						delimLEsc = display[i][0].replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
						//alert(delimLEsc);
						delimREsc = display[i][1].replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
						displayRegexList.push("("+delimLEsc+")((.|[\\r\\n\\t])*?)("+delimREsc+")");
					};
					inlineRegExp = new RegExp(inlineRegexList.join("|"));
					displayRegExp = new RegExp(displayRegexList.join("|"));

					// iterate over pre elements applying RegExp
					// iterate "backwards" as we are removing elements!
					for (i=prelist.length; i>0; i--) {
						if(displayRegExp.test(prelist[i-1].textContent)) {
							var t = document.createTextNode(prelist[i-1].textContent);
							prelist[i-1].parentNode.insertBefore(t,prelist[i-1]);
							prelist[i-1].parentNode.removeChild(prelist[i-1]);
						}
					}
					// iterate over code elements applying RegExp
					// iterate "backwards" as we are removing elements!
					for (i=codelist.length; i>0; i--) {
						if(inlineRegExp.test(codelist[i-1].textContent)) {
							var t = document.createTextNode(codelist[i-1].textContent);
							codelist[i-1].parentNode.insertBefore(t,codelist[i-1]);
							codelist[i-1].parentNode.removeChild(codelist[i-1]);
						}
					}
			  // Now process the page in MathJax
			  MathJax.startup.defaultReady();
			}
		  }
		};
		</script>
		<script type="text/javascript" id="MathJax-script" async
		  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
		</script>
</head>
<body>
	<nav>
        <ul>
            <li>[<a href="/">Home</a>]</li>
            <li>[<a href="/about">About</a>]</li>
            <li>[<a href="/notes">Notes</a>]</li>
			<li>[<a href="/shelf">Shelf</a>]</li>
			<!-- <li>[<a href="/cities">Cities</a>]</li> -->
			<!-- <li>[<a href="/heroes">Heroes</a>]</li> -->
			<li>[<a href="mailto:yudhister.j.kumar@gmail.com">Email</a>]</li>
			<li>[<a href="/atom.xml">RSS</a>]</li>
    </nav>
    <main>
        <h1>Prediction is not Generation</h1>
        <p><i>October 13, 2025</i></p>
        <p>[a long overdue response to <a href="https://aidanjs.com">Aidan</a> :)]</p>
<p>In ML, generation and prediction are practically synonymous. Your model learns an appropriate, performant compression of your dataset, and somehow such artifacts generate "completions" (broadly construed) with high accuracy.</p>
<p>It's tempting to then make the leap to <em>man, if I just managed to tokenize the entire past and future of the universe and train a transformer (with infinite compute) to predict the next universe state every Planck time<sup class="footnote-reference"><a href="#1">1</a></sup> from the universe history up until that point, then it'll be guaranteed to faithfully represent the true laws of physics somewhere in its weights!</em></p>
<p>I claim this is unclear! Even if the laws of physics were describable by some finite state automata, the optimal predictive representation of a process does not have to necessarily correspond to the optimal generative representation!</p>
<p>Here's a toy case. Consider the space of all stationary Markov processes generating the symbol set $\{0,1\}.$ Clearly the best way to predict a process like this (given Markovity) is to assign some probability $p$ to $1$ being generated after a $0,$ and some probability $q$ to $0$ being generated after a $1.$ There are two "belief states" of this policy (let's call them $A$ and $B$â€”each corresponding to the "belief" that $0,1$ will be generated<sup class="footnote-reference"><a href="#2">2</a></sup>) that the reasoner will occupy with probabilities</p>
<p>$$
\mathbb{P}(A) = \frac{1-q}{2-p-q}\, , \, \mathbb{P}(B) = \frac{1-p}{2-p-q}
$$
respectively. The entropy of this two-state system is just the entropy of the stationary distribution (given above), which turns out to be</p>
<p>$$
\begin{align*}
C_\mu &amp;= {-} \mathbb{P}(A) \cdot \log_2 \mathbb{P}(A) - \mathbb{P}(S_1) \cdot \log_2 \mathbb{P}(B) \\<br />
&amp;= \frac{q-1}{2-p-q} \log_2 \left(\frac{1-q}{2-p-q}\right) + \frac{p-1}{2-p-q} \log_2 \left( \frac{1-p}{2-p-q} \right).
\end{align*}
$$</p>
<p>The key point to remember here is that we're using the entropy of the stationary state distribution as a measure of "optimality," in the sense that lower entropy means higher simplicity and as a result it is "more optimal." It stands to reason that if generation and prediction are "the same," then it should be impossible to construct a generative process with lower entropy than $C_\mu$ for some values $p,q.$ Right?</p>
<p>Well. Consider $p = q = 0.4,$ and consider the generating process below.</p>
<img src="/images/Lohr-model.png" alt="Lohr" width="550"/>
<p>You can check for yourself that this process outputs $0 \to 1$ with probability $p,$ and $1 \to 0$ with probability $q$ for $0 \leq p = q \leq 1/2.$ This process has a stationary distribution</p>
<p>$$
\pi = \left[ \frac{1}{2} - p, 2p, \frac{1}{2} - p\right],
$$</p>
<p>and its entropy $H[\pi]$ for $p = q = 0.4$ is approximately $0.922,$ less than $C_\mu = 1.$</p>
<p>Have we been hoodwinked? Maybe one should never trust a sentence beginning with "Clearly, . . ." in a mathematical text. Maybe there's a secret predictive policy that magically has lower entropy for $p \in [0.38, 0.5]$<sup class="footnote-reference"><a href="#3">3</a></sup> that we're just missing.</p>
<p>I argue against this. In particular, there is a particular interpretation of "prediction" we're using here that I claim is simultaneously natural and correct.</p>
<p>Consider an infinite string of tokens $\ldots X_{-2}, X_{-1}, X_0, X_1, X_2, \ldots.$ The Markov property states that $\mathbb{P}(X_0 | X_{-\infty: 0}) = \mathbb{P}(X_0 | X_{-1}):$ that my causal state is fully determined by timestep $T-1,$ and thus the last token output contains all the information I could use to predict the next token output. As an <em>optimal predictor</em>, I want to adhere to the <em>optimal causal policy</em> which is the minimal entropy policy over belief states that can be causally differentiated. In this case, it is the two-state policy $\mu$ with entropy $C_\mu$ above.</p>
<p>Observe that the introduction of causality meaningfully differentiates solely generative policies from predictive ones! We have constructed a lower-entropy generative process by relaxing the assumption that we only rely on meaningfully causally differentiated belief states given the token history. There's a sense in which this is the fundamental difference between prediction and generation. It remains to be seen how widely this holds, but the two concepts are canonically differentiated.</p>
<p><em>Examples taken from <a href="https://csc.ucdavis.edu/~cmg/papers/gmc.pdf">James et. al.</a>.</em></p>
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>Ignoring that the universe doesn't have a sense of absolute time.</p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>In general, belief states are not by default interpretable.</p>
</div>
<div class="footnote-definition" id="3"><sup class="footnote-definition-label">3</sup>
<p>The ranges in which the Lohr model has lower entropy than the predictive model.</p>
</div>

    </main>
</body>
</html>
