<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
        <title>Notes - Page 5</title>
    
    <link rel="stylesheet" href="https://yudhister.me/style.css">
    <!-- MathJax for LaTeX rendering -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://yudhister.me/js/footnotes.js"></script>
    
    <script src="https://yudhister.me/js/filter.js" defer></script>
    
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-TKY9S092B5"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-TKY9S092B5');
    </script>
    <script>
		window.MathJax = {
		  tex: {
			inlineMath: [['$','$'], ['\\(','\\)']],
			displayMath: [ ['$$','$$'], ["\\[","\\]"], ]
		  },
		  startup: {
			ready: () => {
					// Function to iterate over all pre and code elements
					// if they contain TeX/LaTeX code for maths as defined
					// by the markers in tex settings above then copy their
					// textContent before them and remove the element from
					// the DOM.

					// get pre and code elements
					var prelist = document.getElementsByTagName("pre");
					var codelist = document.getElementsByTagName("code");
					// get delimiters for inline and display math
					var inline = MathJax.config.tex.inlineMath;
					var display = MathJax.config.tex.displayMath;
					// start building  a RegExp for each of these math types
					var inlineRegexList = [];
					var displayRegexList =[];
					for(i=0;i<inline.length;i++) {
						// https://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
						delimLEsc = inline[i][0].replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
						//alert(delimLEsc);
						delimREsc = inline[i][1].replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
						inlineRegexList.push("("+delimLEsc+")((.|[\\r\\n\\t])*?)("+delimREsc+")");
					};
					for(i=0;i<display.length;i++) {
						// https://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
						delimLEsc = display[i][0].replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
						//alert(delimLEsc);
						delimREsc = display[i][1].replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
						displayRegexList.push("("+delimLEsc+")((.|[\\r\\n\\t])*?)("+delimREsc+")");
					};
					inlineRegExp = new RegExp(inlineRegexList.join("|"));
					displayRegExp = new RegExp(displayRegexList.join("|"));

					// iterate over pre elements applying RegExp
					// iterate "backwards" as we are removing elements!
					for (i=prelist.length; i>0; i--) {
						if(displayRegExp.test(prelist[i-1].textContent)) {
							var t = document.createTextNode(prelist[i-1].textContent);
							prelist[i-1].parentNode.insertBefore(t,prelist[i-1]);
							prelist[i-1].parentNode.removeChild(prelist[i-1]);
						}
					}
					// iterate over code elements applying RegExp
					// iterate "backwards" as we are removing elements!
					for (i=codelist.length; i>0; i--) {
						if(inlineRegExp.test(codelist[i-1].textContent)) {
							var t = document.createTextNode(codelist[i-1].textContent);
							codelist[i-1].parentNode.insertBefore(t,codelist[i-1]);
							codelist[i-1].parentNode.removeChild(codelist[i-1]);
						}
					}
			  // Now process the page in MathJax
			  MathJax.startup.defaultReady();
			}
		  }
		};
		</script>
		<script type="text/javascript" id="MathJax-script" async
		  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
		</script>
</head>
<body>
	<nav>
        <ul>
            <li>[<a href="/">Home</a>]</li>
            <li>[<a href="/about">About</a>]</li>
            <li>[<a href="/notes">Notes</a>]</li>
			<li>[<a href="/shelf">Shelf</a>]</li>
			<!-- <li>[<a href="/cities">Cities</a>]</li> -->
			<!-- <li>[<a href="/heroes">Heroes</a>]</li> -->
			<li>[<a href="mailto:yudhister.j.kumar@gmail.com">Email</a>]</li>
			<li>[<a href="/atom.xml">RSS</a>]</li>
        </ul>
    </nav>
    <main>
        <h1>Notes</h1>
        
        

        
        <p class="filter-toggle-wrapper">[<a href="#" id="filter-toggle">show all</a>]</p>
        

        
            
            
                
                
                    <article class="post" data-good="false">
                        <header>
                            <h2><a href="https://yudhister.me/mdl-slt/">MDL meets SLT</a></h2>
                            <time datetime="2025-10-14">October 14, 2025</time>
                        </header>
                        <div class="post-content">
                            <p>[paper highlight: <a href="https://arxiv.org/abs/2510.12077v1">Compressibility Measures Complexity: Minimum Description Length Meets Singular Learning Theory</a>]</p>
<p>two major contributions of the paper:</p>
<ul>
<li>theoretically links minimum description length to singular learning theory, in that they prove that for all ground-truth distributions $q$ and $n$ i.i.d samples drawn from $q,$ there exists a two-part code with asymptotic redundancy $R_n = \lambda \log n - (m-1) \log \log n + O_p(1),$ where $\lambda$ is the LLC</li>
<li>experimental results showing LLC variance with model quantization (where quantization is ~roughly a stand-in for compression and LLC measures complexity, so one can study empirical correlations)</li>
</ul>
<p><em>what is a two-part code?</em> admittedly I'm still slightly bamboozled by the MDL formalism they choose, so this will be a mix of hand-wavy intuition and opaque jargon.</p>
<p>Let $q^{(n)} \in \Delta (\mathcal{X}^n)$ be a data-generating distribution over $n$-sequences drawn from the sample space (token vocabulary) $\mathcal{X}.$ Any distribution $p^{(n)}$ over $\mathcal{X}^n$ induces a <em>code</em> for any sample $\bf{x}^{(n)} \in \mathcal{X}^n,$ where a <em>code</em> is just an associated bitstring for the sample. The bitstring has length $- \log p^{(n)}( \bf{x}^{(n)})$ (the entropy), and the minimum description length principle is essentially that good encodings should seek to minimize the minimum average length of samples. Given i.i.d sampling, the long-run optimal encoding distribution <em>is</em> the ground-truth distribution $q^{(n)},$ and $\text{KL}(q||p)$ has a clean interpretation in this context: the expected excess length per-symbol given by encoding distribution $p$ vs. $q.$</p>
<p>a two-part code is an encoding with two parts: one specifying the encoding distribution ("model") with respect to a model class, and the other specifying the message ("sample") given the model. intuition for this setup: imagine a sender and receiver having mutual information over the fact that both will communicate via some language with some grammatical structure, but the structure insufficiently specifies a full language and vocabulary, <em>however</em> they both have a dictionary mapping bitstrings to complete languages that they can coordinate on first before communicating. (there are much better ways of explaining this).</p>
<p>anyway, you want some way of measuring the performance of your encoding in the two-part setting. there's a quantity called <em>redundancy</em> that measures performance with regards to the underlying data distribution, roughly given by
$$
R_n = \text{len}([[p]]) + \text{KL}(q || p),
$$
in the average case, where $[[p]]$ is your bitstring encoding of your model w.r.t. your model class. a natural way of optimizing this is choosing a $p$ which accurately models $q$ and eating the specification cost. However! you might have a  model class $\mathcal{M}$ uniquely unsuited to encoding $q,$ in which case your optimization problem is more interesting.</p>
<p>restating the central theoretical result: there exists a two-part code for any realizable<sup class="footnote-reference"><a href="#1">1</a></sup> data generating distribution $q \in \mathcal{M}$ and dataset $\bf{x}^{(n)}$ sampled i.i.d from $q,$ the asymptotic redundancy is
$$
R_n = \lambda \log n - (m - 1) \log \log n + O_p(1),
$$
where $\lambda$ is the LLC of $q$ and $m$ is the "multiplicity."<sup class="footnote-reference"><a href="#1">1</a></sup></p>
<p>it is late and my brain is not quite working, but i don't see optimality guarantees for this result? the construction is of the flavor "choose codes such that</p>
<p>$$\text{len}([[p^*]]) = \log \frac{\text{Vol}(W)}{V_{p^*_n}(\epsilon)},$$</p>
<p>and then this has $R_n$ given above." (where $p^*$ are the model encodings at the center of $\epsilon$-balls covering model regions with sufficiently small KL divergence, necessary because discretization is needed to reduce to a set small enough to fully specify with codes). like, this is implicitly sane because $\text{KL}$ $\epsilon$-balls partition assigns probability proportional to the share of the $\epsilon$-ball vs. the volume of the entire space, but IDK why is this the optimal encoding or agreement algorithm? <em>mumbles in Jeffrey's prior</em></p>
<p>a maybe helpful image:</p>
<img src="/images/degeneracy-compressibility.png" alt="pythia-llc" width="550"/>
<p>I suppose this is why the empirical results are needed. But the empirical results are like "linear relationships between LLC estimates and critical compression thresholds for models up to 7B parameters" where the critical compression thresholds $n_q$ are literally "how many times do I need to quantize the model before the difference in loss exceeds some threshold." which is cool! but a bit confusing</p>
<img src="/images/pythia-quantization.png" alt="pythia-llc" width="550"/>
<p>still don't quite understand the theory behind LLC estimation. MDL and SLT connections are cool though, it would be nice to get some naturality results bc the experimental results are not that convincing by themselves (many patterns replicate this, LLC estimation is an art not a science, and quantizing models arbitrarily and doing inference on them seems like it naturally leads to buggy implementations)</p>
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>see technical conditions in the paper</p>
</div>

                        </div>
                    </article>
                    
                        <hr class="post-separator">
                    
                
                    <article class="post" data-good="false">
                        <header>
                            <h2><a href="https://yudhister.me/prediction-is-not-generation/">Prediction is not Generation</a></h2>
                            <time datetime="2025-10-13">October 13, 2025</time>
                        </header>
                        <div class="post-content">
                            <p>[a long overdue response to <a href="https://aidanjs.com">Aidan</a> :)]</p>
<p>In ML, generation and prediction are practically synonymous. Your model learns an appropriate, performant compression of your dataset, and somehow such artifacts generate "completions" (broadly construed) with high accuracy.</p>
<p>It's tempting to then make the leap to <em>man, if I just managed to tokenize the entire past and future of the universe and train a transformer (with infinite compute) to predict the next universe state every Planck time<sup class="footnote-reference"><a href="#1">1</a></sup> from the universe history up until that point, then it'll be guaranteed to faithfully represent the true laws of physics somewhere in its weights!</em></p>
<p>I claim this is unclear! Even if the laws of physics were describable by some finite state automata, the optimal predictive representation of a process does not have to necessarily correspond to the optimal generative representation!</p>
<p>Here's a toy case. Consider the space of all stationary Markov processes generating the symbol set $\{0,1\}.$ Clearly the best way to predict a process like this (given Markovity) is to assign some probability $p$ to $1$ being generated after a $0,$ and some probability $q$ to $0$ being generated after a $1.$ There are two "belief states" of this policy (let's call them $A$ and $B$—each corresponding to the "belief" that $0,1$ will be generated<sup class="footnote-reference"><a href="#2">2</a></sup>) that the reasoner will occupy with probabilities</p>
<p>$$
\mathbb{P}(A) = \frac{1-q}{2-p-q}\, , \, \mathbb{P}(B) = \frac{1-p}{2-p-q}
$$
respectively. The entropy of this two-state system is just the entropy of the stationary distribution (given above), which turns out to be</p>
<p>$$
\begin{align*}
C_\mu &amp;= {-} \mathbb{P}(A) \cdot \log_2 \mathbb{P}(A) - \mathbb{P}(S_1) \cdot \log_2 \mathbb{P}(B) \\<br />
&amp;= \frac{q-1}{2-p-q} \log_2 \left(\frac{1-q}{2-p-q}\right) + \frac{p-1}{2-p-q} \log_2 \left( \frac{1-p}{2-p-q} \right).
\end{align*}
$$</p>
<p>The key point to remember here is that we're using the entropy of the stationary state distribution as a measure of "optimality," in the sense that lower entropy means higher simplicity and as a result it is "more optimal." It stands to reason that if generation and prediction are "the same," then it should be impossible to construct a generative process with lower entropy than $C_\mu$ for some values $p,q.$ Right?</p>
<p>Well. Consider $p = q = 0.4,$ and consider the generating process below.</p>
<img src="/images/Lohr-model.png" alt="Lohr" width="550"/>
<p>You can check for yourself that this process outputs $0 \to 1$ with probability $p,$ and $1 \to 0$ with probability $q$ for $0 \leq p = q \leq 1/2.$ This process has a stationary distribution</p>
<p>$$
\pi = \left[ \frac{1}{2} - p, 2p, \frac{1}{2} - p\right],
$$</p>
<p>and its entropy $H[\pi]$ for $p = q = 0.4$ is approximately $0.922,$ less than $C_\mu = 1.$</p>
<p>Have we been hoodwinked? Maybe one should never trust a sentence beginning with "Clearly, . . ." in a mathematical text. Maybe there's a secret predictive policy that magically has lower entropy for $p \in [0.38, 0.5]$<sup class="footnote-reference"><a href="#3">3</a></sup> that we're just missing.</p>
<p>I argue against this. In particular, there is a particular interpretation of "prediction" we're using here that I claim is simultaneously natural and correct.</p>
<p>Consider an infinite string of tokens $\ldots X_{-2}, X_{-1}, X_0, X_1, X_2, \ldots.$ The Markov property states that $\mathbb{P}(X_0 | X_{-\infty: 0}) = \mathbb{P}(X_0 | X_{-1}):$ that my causal state is fully determined by timestep $T-1,$ and thus the last token output contains all the information I could use to predict the next token output. As an <em>optimal predictor</em>, I want to adhere to the <em>optimal causal policy</em> which is the minimal entropy policy over belief states that can be causally differentiated. In this case, it is the two-state policy $\mu$ with entropy $C_\mu$ above.</p>
<p>Observe that the introduction of causality meaningfully differentiates solely generative policies from predictive ones! We have constructed a lower-entropy generative process by relaxing the assumption that we only rely on meaningfully causally differentiated belief states given the token history. There's a sense in which this is the fundamental difference between prediction and generation. It remains to be seen how widely this holds, but the two concepts are canonically differentiated.</p>
<p><em>Examples taken from <a href="https://csc.ucdavis.edu/~cmg/papers/gmc.pdf">James et. al.</a>.</em></p>
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>Ignoring that the universe doesn't have a sense of absolute time.</p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>In general, belief states are not by default interpretable.</p>
</div>
<div class="footnote-definition" id="3"><sup class="footnote-definition-label">3</sup>
<p>The ranges in which the Lohr model has lower entropy than the predictive model.</p>
</div>

                        </div>
                    </article>
                    
                        <hr class="post-separator">
                    
                
                    <article class="post" data-good="false">
                        <header>
                            <h2><a href="https://yudhister.me/poetry-1/">Miscellaneous Poetry Drafts</a></h2>
                            <time datetime="2025-10-12">October 12, 2025</time>
                        </header>
                        <div class="post-content">
                            <p><strong>I.</strong></p>
<p>a blade of grass hides<br />
minuscule migratory men<br />
Lilliputian fiends</p>
<p><strong>II.</strong></p>
<p>mighty merry rascals<br />
fickle, high off foglefreude<br />
die English skippers</p>
<p><strong>III.</strong></p>
<p>I once beheld Seneca's estate,<br />
Credulously inspecting his wicker tomb—<br />
Yet Margate Mennons and liced defendants<br />
Both swore by its awkward loom.</p>
<p>My heart gasped and lips shuddered<br />
When, to my utmost surprise<br />
The elderly Roman statesman lay<br />
As mummified nitride.</p>
<p>Tick-tock, goes the clock<br />
Garrulous gyrations too<br />
Gizzardly Gentry, nice surprise<br />
Confiding in a martyr's womb.</p>
<p>Foiled! the cuckoo's dead—<br />
Not I, not I, not I!<br />
Lambastation! Aberration!<br />
To defy Nero's evil eye.</p>
<p><strong>IV.</strong></p>
<p>weed stands stout<br />
thistles, burr<br />
gadolinum kraut<br />
hummus and herb</p>
<p>olden spires bristle, copper-waxed<br />
kelpish tides awash blades of glass<br />
Betty stout, orange'd brass<br />
vermillion mounts, dugong grass</p>
<p>betwixed, witched, yonder<br />
your shivers roll down my spine<br />
I gave you a bouquet of thorned tulips<br />
at sunrise, on Cocoa Beach</p>
<p>the rocket's red glare, the bombs<br />
bursting over a lackadaiscal mare<br />
I wish Martians dreamt of the stars<br />
alas</p>

                        </div>
                    </article>
                    
                        <hr class="post-separator">
                    
                
                    <article class="post" data-good="false">
                        <header>
                            <h2><a href="https://yudhister.me/scaling-transfer-1/">Scaling Laws for Transfer Learning</a></h2>
                            <time datetime="2025-10-11">October 11, 2025</time>
                        </header>
                        <div class="post-content">
                            <p>Chinchilla scaling posits that</p>
<p>$$
L(N,D) = L_{\infty} + AN^{-\alpha} + BD^{-\beta},
$$</p>
<p>where $N$ is the number of parameters in your language model, $D$ is the number of training tokens seen, $A,B,\alpha,\beta, L_{\infty}$ are constants, and $L(N,D)$ is the test loss.</p>
<p>Of course, this analysis is limited:</p>
<ul>
<li>parameters do not hold across architectural shifts: <a href="https://arxiv.org/abs/2501.12370">dense vs. MoE for ex</a>. (h/t <a href="https://kushalthaman.github.io/">Kushal</a> for sending me this paper)</li>
<li>"scale data and model size similarly" is derived from the regime where compute $C \propto ND,$</li>
<li>data repetition may or may not degrade performance in the long-term: it seems like <a href="https://arxiv.org/abs/2305.16264">4x is the limit</a> for traditional autoregressive transformers, but <a href="https://arxiv.org/abs/2507.15857">100x can be useful for diffusion LMs</a></li>
<li>$L(N,D)$ is in-distribution test loss, loss often not predictive of downstream task performance, although <a href="https://arxiv.org/abs/2411.12925v1">loss-to-loss predictions across different training distributions are predictable</a></li>
<li><a href="https://epoch.ai/blog/chinchilla-scaling-a-replication-attempt">the original Chinchilla paper likely did their regressions wrong</a></li>
</ul>
<p>Still, it is remarkable that test loss performance is so clearly a function of parameters and dataset size, with little assumptions made about the distributional structure or architectural specifics (this holds across varying depth/width ratios, for instance). I find two questions natural:</p>
<ul>
<li>does scaling hold outside of the cross-entropy pretraining regime?</li>
<li>can we derive scaling relationships for downstream task performance? in particular, how predictable is transfer learning?</li>
</ul>
<p>In 2021, OpenAI <a href="https://arxiv.org/abs/2102.01293">studied the question</a> of "how much more quickly does a pretrained LM achieve low loss on a finetuning dataset than an LM initialized from scratch on the dataset?" (Note pretraining and finetuning on this case are "the same operation", the objective is still cross-entropy). They find that the "effective data transferred"<sup class="footnote-reference"><a href="#1">1</a></sup> $D_T$ is described by
$$
D_T = k(D_F)^\alpha (N)^\beta,
$$
where $D_F$ is the size of the finetuning dataset (in tokens) and $N$ is the number of non-embedding parameters of the model.<sup class="footnote-reference"><a href="#2">2</a></sup> This is great! Strong evidence of the generality of abstractions the model learns in pretraining (especially given the independence of $\beta$ from the source distribution). However, it doesn't explicitly tell us about downstream task performance given an external metric.</p>
<p><a href="https://arxiv.org/abs/2411.12925v1">Brandfonbrener et. al. do somewhat better</a>. They derive scaling relationships for train-train, train-test, and test-test loss transfer for models trained on different datasets, which can be expressed as</p>
<p>$$L_i(\hat{f}_j^{N,D}) \approx K \cdot \left( L_k(f_l^{N,D}) - E_{k | l}\right)^\kappa + E_{i|j},$$</p>
<p>where you have models $f_j, f_l$ trained on distributions $j,l$ evaluated on distributions $i, k$ and you're fitting the constants $K, \kappa.$<sup class="footnote-reference"><a href="#3">3</a></sup> As an example, the case of train-train would be where $(i,j) = (0,0)$ and $(k, l) = (1,1).$ We pair models by $(N,D)$ for coherence. Notably, these laws hold for diverse datasets, but only well in low-loss regimes and when $E_{m|n}$ terms can be well estimated. Still no breaking of the pretraining regime, and no explicit predictions for downstream metric performance!</p>
<p><a href="https://arxiv.org/abs/2507.00885">There's a meta-analysis out this year that claims that scaling laws are unreliable for dowstream task performance prediction.</a>. Seems correct. Metrics are noisy and don't have nice algorithmic properties like cross-entropy loss might. Perhaps intriguing is their observation that irregular scaling is (1) common and (2) can occur for cross-entropy on normal tasks and normal LM datasets. <a href="https://proceedings.mlr.press/v202/liu23ao/liu23ao.pdf">This paper claims that larger models &amp; models trained for longer have better downstream task performance even when holding loss constant.</a> Which is an argument for certain training setups &amp; architectures having better inductive biases?</p>
<p>Honestly, I am kind of sad that the extant literature here seems to be tainted by publication bias? I wouldn't really trust these papers (or the ten others I read writing this), and I want to run the experiments myself. The Tinker API seems good for doing this quickly. I'll go do that.</p>
<p>(Titular question pending.)</p>
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>In essence, the number of tokens that you "save" seeing in finetuning by pretraining.</p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>Notably $\beta$ only depends on architecture and TARGET distribution (not SOURCE), while $\alpha$ is a rough "distributional proximity" proxy that can be easily estimated.</p>
</div>
<div class="footnote-definition" id="3"><sup class="footnote-definition-label">3</sup>
<p>$E_{m|n}$ is the irreducible loss of a model trained on with infinite compute on distribution $n$ evaluated on distribution $m.$</p>
</div>

                        </div>
                    </article>
                    
                        <hr class="post-separator">
                    
                
                    <article class="post" data-good="false">
                        <header>
                            <h2><a href="https://yudhister.me/safety-cases/">On Non-Isolated Calls for Structure</a></h2>
                            <time datetime="2025-09-26">September 26, 2025</time>
                        </header>
                        <div class="post-content">
                            <p>Safety cases are arguments that AI deployments are safe in some specified context. The context can include restrictions on deployment environments as well as training or deployment protocols. For instance, <a href="https://arxiv.org/abs/2505.03989">the debate safety case</a> only applies to low-stakes deployment environments, requires exploration guarantees on the model, and relies on a debate protocol which avoids obfuscated arguments. Given these assumptions, Buhl et. al. argue for “asymptotic guarantees”—that high performance on alignment objectives during training translate to approximate alignment during deployment. <a href="https://arxiv.org/abs/2501.17315">The control safety case</a> is structurally similar, instead focusing directly on an explicit threat model and concretizing assumptions accordingly.</p>
<p>A naive way of constructing an “alignment portfolio” is simply to make safety cases which adequately cover all deployment environments with the appropriate degree of risk-tolerance. Formal verification for high-stakes SWE deployment, white-box interpretability for monitoring automated alignment researchers, some adapted debate protocol for use in executive decision-making. If the individual arguments are all sound, this works!</p>
<p>What if we introduce some error into the soundness judgements? If every safety case has some epsilon probability of failure, then straightforwardly you should make more safety cases for the scenarios in which alignment properties matter more. But if all your safety cases for non-deceptive automated alignment researchers rely on “white-box interpretability mostly working,” then if this isn’t true you’re still doomed no matter how many safety cases you write!</p>
<p><a href="https://alignment.anthropic.com/2024/safety-cases/">Anthropic’s ASL-4 safety case sketches</a> are not quite this correlated, but only just. [1] relies on white-box methods successfully monitoring deception, [3] relies on guarantees that the pretrained model is not coherently deceptive (likely requiring successful white-box or black-box methods), and [2] still depends on linear activation probes adequately showing that the model cannot distinguish between certain classes of train and test deployments, as well as black-box evaluations providing sufficiently robust guarantees on behavior. These are similar assumptions! These assumptions are all only true in worlds where “models are sufficiently parsimonious such that present-day interpretability techniques and evals can provide rigorous guarantees on good behavior.”</p>
<p>In general, <em>insufficient diversity over the world structure assumed in an alignment portfolio makes the portfolio fragile and irrobust</em>.<sup class="footnote-reference"><a href="#1">1</a></sup></p>
<p>It is always necessary to make assumptions about world structure when predicting world behavior. A bounded reasoner simulates the world with a local, low-fidelity model based on the reasoner’s accumulated evidence about the world. Some assumptions on world structure are better than others—gravity following an inverse-square law vs. homeopathic remedies curing cancer, for instance.</p>
<p>Considering the structure of one’s structural assumptions is critically important in domains where the world behavior has not been exhibited and it is of importance. Note:</p>
<ul>
<li>The largest scientific breakthroughs are accompanied by structural assumptions about the world breaking. See the atomic bomb, CRISPR, heavier-than-air flight. Fundamentally, these “expand the domain of the possible.” Sometimes, the world structure is discovered first (as in nuclear theory leading to the first controlled chain reaction). Other times, a prototype uncovers the structure (see: penicillin). In both cases, the non-specialist intelligent reasoner understands a different possibility domain before and after.</li>
<li>Top-down searches for structural guarantees must be incredibly judicious in their assumptions, because the vast majority of hypotheses are incorrect. Ex post, the structure is obvious, but ex ante it is not. Consider Newton devoting as much energy to alchemy as the study of gravitation.</li>
<li>If we take the perspective that <a href="https://www.lesswrong.com/posts/nkeYxjdrWBJvwbnTr/an-advent-of-thought">alignment is an infinite problem</a>, there is no good reason to expect that the world structure we can reasonably assume is simple. It might be that it is infinitely complex and is only limited by our current understanding, and that we will recover finer and finer approximations of it as our understanding improves. At each stage of this process we will have to repeat our assumption examination from a roughly equivalent epistemic vantage point of staring into the abyss.</li>
<li>Much of the existential risk from AI development comes from tail risks and black swan events. Mitigating these requires a portfolio of solutions which each rely on decorrelated or independent world models (note this is not a guarantee).</li>
</ul>
<p>Natural corollaries of this observation:</p>
<ul>
<li>we should be explicit about which world models are going into constructing safety cases,</li>
<li>we should be developing independent safety cases for high-stakes deployment situations,</li>
<li>we should emphasize diversity in theoretical agendas to buttress our ability to make such safety cases reliant on disjoint sets of assumptions.</li>
</ul>
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>This is a specific instance of the general case of “Swiss cheese models only work when the holes don’t line up in the same worlds,” which is probably not sufficiently justified in this post but is something I believe to be true.</p>
</div>

                        </div>
                    </article>
                    
                

                
                <nav class="pagination">
                    
                        <a class="previous" href="https://yudhister.me/notes/page/4/">‹ Previous</a>
                    
                    <span class="page-number">Page 5 of 11</span>
                    
                        <a class="next" href="https://yudhister.me/notes/page/6/">Next ›</a>
                    
                </nav>

            
        
    </main>
</body>
</html>
