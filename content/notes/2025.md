---
title: "2025 In Review"
author: Yudhister Kumar
date: 2026-01-01
path: "2025"
extra:
  good: true
---

2025 was odd. 

It was the best of times. I am happier today than I ever have been. I learned gorgeous and utilitarian concepts alike; I crossed the employability threshold and work with incredible comrades; I live in a home with some of my favorite people on the planet. I'll be 20 soon and the future is bright.

It was the worst of times. Mired in a haze of hopelessness and confusion. I spent months and months in pain and as an insomniac; I was cripplingly sick for half a year. For the first time, I have regrets. Proper ones, ones that I'll never forget as long as I live. 

It is good to have lived, maybe. I never want to let the preconditions for this year exist again. 

<h3>Ideas</h3>

Spent the first half of the year thinking about how to think about neural networks. Mean-field approaches to spin glasses, nonequilibrium QFT, KL bounds via stochastic coupling, depth-width tradeoffs, representational capacity (benchmarked by circuit complexity classes), singular learning theory, computational mechanics, variational reformulations of RL. Some of it was misguided, much was interesting. 

Vaintrob et. al. have been pursuing a QFT-inspired approach to interpretability. It's really cool, and I'm excited to (hopefully) see their empirical work come out this year. Dmitry's post on ["SLT as a thermodynamic theory of Bayesian learning, but not the thermodynamic theory of Bayesian learning"](https://www.lesswrong.com/posts/M2bs6xCbmc79nwr8j/dmitry-vaintrob-s-shortform?commentId=A8Ziwhts35dgqbz52) is one of the most interesting I've read this year. 

Read Debreu on microeconomics. Yasheng Huang on Chinese capitalism. Ran a Land-focused reading group running through Bateson, Hegel, analytical Marxism, Kant, and Land himself. Finnegans Wake and EGA are both prime examples of [idiolects](https://www.yudhister.me/idiolects/). Yuxi Liu has a great blog. Read the formalization of Chomsky's syntax-semantics theory while recovering from surgery. Thought about chromosomal selection and inducing meiosis in oogonia at the Reproductive Frontiers conference. 

Summer rolled around, I was briefly back to 95% capacity, and I was contracting for ILIAD (the org) & planning to intern at Softmax. Making an open problems list for ODYSSEY (the conference) was enlightening. Curation is very, very difficult! Making sure that your theoretical brainchildren touch grass is almost as hard! 

Softmax forced me to properly learn to code. Programming is very different when done collaboratively in a shared codebase, doubly so when one's research code must be written quickly, efficiently, and in a manner interpretable to others. I'm glad I got to think about multi-agent RL and what makes good management good. 

My thoughts for the rest of the year were less legible. Tiling is an important problem. Formalizing it is hard. Acausal coalitional structure is confusing. Astronomical waste may or may not be an issue. Transpiling meta-ethical frameworks is hard. Biosingularitarian governance is hard. Is macrohistory determined by the nucleation stages of technological development or by deep convergent pressures leading to certain outcomes? How would we know?

At SI, we think about meta-learning and recurrence, among other things. Attempting to understand these deeply has been fruitful. A Berkeley professor ran a seminar on Adorno and poetry with a wonderful reading list. Semiotic physics is confusing, but Owain Evans has some good frames. Evolutionary optimization is surprisingly sample-efficient. Training deep neural networks is hard. Post-AGI futures are confusing and hard to think about, but Korinek has good frames. Weight-sparse models are deeply interesting. 

In 2026, I want to drastically shorten my map-territory feedback loops. Intellectually, my greatest flaw this year was not investing in it. Admittedly, I probably did not have the energy for it. Luckily, this year is shaping up to be different.  

<h3>Some Randomly Sampled Moments</h3>

[redacted]

<h3> People </h3>

[redacted]

<h3>Miscellanea</h3> 

[TBD]































